{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145f2861-ada2-464b-87e1-ccd04eec71be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lazy_nlp_pipeline import NLP, Pattern as P, TokenPattern as TP, WordPattern as WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6eaa70-a719-448b-b63e-2135e3cfe2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = NLP(project_name='example_patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3610460f-a832-4206-8d9d-090701ae3780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1 a')[0:3 doc=140483887639776]\n",
      "Span('1 a')[10:13 doc=140483887639392]\n"
     ]
    }
   ],
   "source": [
    "# Sequence of tokens\n",
    "\n",
    "pattern = P(\n",
    "    TP('1'),\n",
    "    TP('a'),\n",
    ")\n",
    "\n",
    "\n",
    "test_texts = [\n",
    "    '1 a',\n",
    "    'Something 1 a something',\n",
    "    'Something Something2',\n",
    "]\n",
    "\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fbefc1-6b6a-4598-9256-2a653ffb0789",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1 2 a 1 2')[0:9 doc=140483887629360]\n",
      "Span('1 2 a 1 2')[10:19 doc=140483887633008]\n"
     ]
    }
   ],
   "source": [
    "# subpatterns\n",
    "\n",
    "p1 = P(\n",
    "    TP('1'),\n",
    "    TP('2'),\n",
    ")\n",
    "\n",
    "pattern = P(\n",
    "    p1,\n",
    "    TP('a'),\n",
    "    p1,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1 2 a 1 2',\n",
    "    'Something 1 2 a 1 2 something:::a:b',\n",
    "    'Something Something2',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9ac417-156c-4b75-9324-f5a3042f63ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1:0::0')[0:6 doc=140483887638816]\n",
      "Span('1:::::0:0')[10:19 doc=140483887639248]\n"
     ]
    }
   ],
   "source": [
    "# allow_inbetween\n",
    "\n",
    "pattern = P(\n",
    "    TP('1'),\n",
    "    TP('0'),\n",
    "    TP('0'),\n",
    "    \n",
    "    allow_inbetween=TP(':'),\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1:0::0',\n",
    "    'Something 1:::::0:0',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473d1c70-da1f-4e48-b74f-683191eb91c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('e2 e4')[0:5 doc=140483887643856]\n"
     ]
    }
   ],
   "source": [
    "# allow_inbetween=None\n",
    "\n",
    "pattern = P(\n",
    "    TP('e'),\n",
    "    TP('2'),\n",
    "    TP(' '),\n",
    "    TP('e'),\n",
    "    TP('4'),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    'e2 e4',\n",
    "    'e 2 e 4',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1935a173-c5f0-4733-b7ea-054ceb60f035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('From 2001-01-10 to 2009-01-10')[0:29 doc=140483890578960]\n",
      "Span('2001-01-10 to 2009-01-10')[5:29 doc=140483890578960]\n",
      "Span('10-01-2001 to 2009-01-10')[10:34 doc=140483887634784]\n"
     ]
    }
   ],
   "source": [
    "# date example\n",
    "\n",
    "ymd_date = P(\n",
    "    TP(isnumeric=True, min_len=4, max_len=4),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "dmy_date = P(\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=4, max_len=4),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "pattern = P(\n",
    "    TP('from', ignore_case=True)[0:1],\n",
    "    ymd_date | dmy_date,\n",
    "    TP('to', ignore_case=True),\n",
    "    ymd_date | dmy_date,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1999-01-10',\n",
    "    'From 2001-01-10 to 2009-01-10',\n",
    "    'Something 10-01-2001 to 2009-01-10 Something2',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "485b99a2-9c63-4b2e-b290-87f28bd96b94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('неприбуткова організація')[125:149 doc=140483887642896]\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "pattern = P(\n",
    "    WP(lemma='неприбутковий'),\n",
    "    WP()[1:],\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    'Вікіпе́дія (англ. Wikipedia, МФА: [ˌwɪkɪˈpiːdɪə]) — загальнодоступна вільна '\n",
    "    'багатомовна онлайн-енциклопедія, якою опікується неприбуткова організація «Фонд Вікімедіа».',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed46c5e-35ff-47c4-9cda-653b31a37c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('загальнодоступна вільна')[52:75 doc=140483887629984]\n",
      "Span('багатомовна онлайн')[76:94 doc=140483887629984]\n",
      "Span('багатомовна онлайн-енциклопедія')[76:107 doc=140483887629984]\n",
      "Span('неприбуткова організація')[125:149 doc=140483887629984]\n"
     ]
    }
   ],
   "source": [
    "# pos, note false positives\n",
    "\n",
    "pattern = P(\n",
    "    WP(pos='ADJF'),\n",
    "    WP(pos='NOUN'),\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    'Вікіпе́дія (англ. Wikipedia, МФА: [ˌwɪkɪˈpiːdɪə]) — загальнодоступна вільна '\n",
    "    'багатомовна онлайн-енциклопедія, якою опікується неприбуткова організація «Фонд Вікімедіа».',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixenv",
   "language": "python",
   "name": "mixenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
