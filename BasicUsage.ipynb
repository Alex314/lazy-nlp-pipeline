{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145f2861-ada2-464b-87e1-ccd04eec71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazy_nlp_pipeline import NLP, Pattern as P, TokenPattern as TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6eaa70-a719-448b-b63e-2135e3cfe2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP(project_name='example_patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3610460f-a832-4206-8d9d-090701ae3780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|██████████████████████████████████████████████| 3/3 [00:00<00:00, 13301.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1 a')[0:3 doc=139779671233552]\n",
      "Span('1 a')[10:13 doc=139779647797072]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sequence of tokens\n",
    "\n",
    "pattern = P(\n",
    "    TP('1'),\n",
    "    TP('a'),\n",
    ")\n",
    "\n",
    "\n",
    "test_texts = [\n",
    "    '1 a',\n",
    "    'Something 1 a something',\n",
    "    'Something Something2',\n",
    "]\n",
    "\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9fbefc1-6b6a-4598-9256-2a653ffb0789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|████████████████████████████████████████████████| 3/3 [00:00<00:00, 925.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1 2 a 1 2')[0:9 doc=139779647674704]\n",
      "Span('1 2 a 1 2')[10:19 doc=139779665671440]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# subpatterns\n",
    "\n",
    "p1 = P(\n",
    "    TP('1'),\n",
    "    TP('2'),\n",
    ")\n",
    "\n",
    "pattern = P(\n",
    "    p1,\n",
    "    TP('a'),\n",
    "    p1,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1 2 a 1 2',\n",
    "    'Something 1 2 a 1 2 something:::a:b',\n",
    "    'Something Something2',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d9ac417-156c-4b75-9324-f5a3042f63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|███████████████████████████████████████████████| 2/2 [00:00<00:00, 7503.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('1:0::0')[0:6 doc=139779665376656]\n",
      "Span('1:::::0:0')[10:19 doc=139779671096464]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# allow_inbetween\n",
    "\n",
    "pattern = P(\n",
    "    TP('1'),\n",
    "    TP('0'),\n",
    "    TP('0'),\n",
    "    \n",
    "    allow_inbetween=TP(':'),\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1:0::0',\n",
    "    'Something 1:::::0:0',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "473d1c70-da1f-4e48-b74f-683191eb91c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|███████████████████████████████████████████████| 2/2 [00:00<00:00, 6748.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('e2 e4')[0:5 doc=139779665673936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# allow_inbetween=None\n",
    "\n",
    "pattern = P(\n",
    "    TP('e'),\n",
    "    TP('2'),\n",
    "    TP(' '),\n",
    "    TP('e'),\n",
    "    TP('4'),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    'e2 e4',\n",
    "    'e 2 e 4',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1935a173-c5f0-4733-b7ea-054ceb60f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|███████████████████████████████████████████████| 3/3 [00:00<00:00, 2564.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('From 2001-01-10 to 2009-01-10')[0:29 doc=139779647872912]\n",
      "Span('2001-01-10 to 2009-01-10')[5:29 doc=139779647872912]\n",
      "Span('10-01-2001 to 2009-01-10')[10:34 doc=139779647872080]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# date example\n",
    "\n",
    "ymd_date = P(\n",
    "    TP(isnumeric=True, min_len=4, max_len=4),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "dmy_date = P(\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=2, max_len=2),\n",
    "    TP('-'),\n",
    "    TP(isnumeric=True, min_len=4, max_len=4),\n",
    "    \n",
    "    allow_inbetween=None,\n",
    ")\n",
    "\n",
    "pattern = P(\n",
    "    TP('from', ignore_case=True)[0:1],\n",
    "    ymd_date | dmy_date,\n",
    "    TP('to', ignore_case=True),\n",
    "    ymd_date | dmy_date,\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    '1999-01-10',\n",
    "    'From 2001-01-10 to 2009-01-10',\n",
    "    'Something 10-01-2001 to 2009-01-10 Something2',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "485b99a2-9c63-4b2e-b290-87f28bd96b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "match_patterns: 100%|█████████████████████████████████████████████████| 1/1 [00:00<00:00, 16.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span('общедоступная интернет')[30:52 doc=139779665672400]\n",
      "Span('общедоступная интернет-')[30:53 doc=139779665672400]\n",
      "Span('общедоступная интернет-энциклопедия')[30:65 doc=139779665672400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Russian lemmatization\n",
    "\n",
    "pattern = P(\n",
    "    TP(lemma='общедоступный'),\n",
    "    TP(isspace=False)[1:],\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    'Википедия (англ. Wikipedia) — общедоступная интернет-энциклопедия реализованная на принципах вики',\n",
    "]\n",
    "for span in nlp.match_patterns([pattern], texts=test_texts):\n",
    "    print(span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixenv3.11",
   "language": "python",
   "name": "mixenv3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
